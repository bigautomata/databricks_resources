To join the `system.lakeflow.pipeline_update_timeline` metadata with billing usage and billing list prices metadata in Databricks, you need to:

- Use `pipeline_id` from `pipeline_update_timeline` and `usage_metadata.dlt_pipeline_id` from `system.billing.usage` as the join key.
- Join `system.billing.usage` with `system.billing.list_prices` using `sku_name` and the relevant time range.
- Filter for `billing_origin_product='DLT'` to focus on declarative pipelines.

Here’s an example SQL query that demonstrates this join:

```sql
SELECT
  t1.workspace_id,
  t1.pipeline_id,
  t1.update_id,
  t1.period_start_time,
  t1.period_end_time,
  t1.result_state,
  t2.usage_date,
  t2.sku_name,
  t2.usage_quantity,
  t2.billing_origin_product,
  t3.pricing.default AS list_price,
  t2.usage_quantity * t3.pricing.default AS estimated_cost
FROM system.lakeflow.pipeline_update_timeline t1
LEFT JOIN system.billing.usage t2
  ON t1.workspace_id = t2.workspace_id
  AND t1.pipeline_id = t2.usage_metadata.dlt_pipeline_id
  AND t2.billing_origin_product = 'DLT'
  AND t2.usage_date BETWEEN to_date(t1.period_start_time) AND to_date(t1.period_end_time)
LEFT JOIN system.billing.list_prices t3
  ON t2.sku_name = t3.sku_name
  AND t2.usage_end_time >= t3.price_start_time
  AND (t3.price_end_time IS NULL OR t2.usage_end_time < t3.price_end_time)
WHERE t1.period_start_time > CURRENT_TIMESTAMP() - INTERVAL 30 DAYS
```
This query:
- Retrieves pipeline update timeline metadata.
- Joins with billing usage records for DLT pipelines.
- Joins with list price metadata to calculate estimated cost.
- Filters for updates in the last 30 days.

You can further customize the query to include additional columns or filter for specific pipelines or users[[1]](https://docs.databricks.com/aws/en/ldp/monitor-event-logs/ "/docs.databricks.com/aws/en/ldp/monitor-event-logs/")[[2]](https://docs.databricks.com/aws/en/admin/usage/system-tables/ "/docs.databricks.com/aws/en/admin/usage/system-tables/").

To join the `system.lakeflow.pipeline_update_timeline` metadata with billing usage and billing list prices metadata using pandas, you should:

- Use `workspace_id` and `pipeline_id` from `pipeline_update_timeline_df` and `usage_metadata.dlt_pipeline_id` from `billing_usage_df` as join keys.
- Join `billing_usage_df` with `billing_list_prices_df` using `sku_name` and `cloud`.
- Filter for relevant time ranges and product types (e.g., `billing_origin_product == 'DLT'`).

Here is optimized pandas code aligned with Databricks best practices:

```python
import pandas as pd

# Filter billing usage for DLT pipelines
billing_usage_dlt = billing_usage_df[
    billing_usage_df['billing_origin_product'] == 'DLT'
]

# Join pipeline_update_timeline with billing_usage
merged = pd.merge(
    pipeline_update_timeline_df,
    billing_usage_dlt,
    left_on=['workspace_id', 'pipeline_id'],
    right_on=['workspace_id', 'usage_metadata.dlt_pipeline_id'],
    how='inner'
)

# Join with billing_list_prices on sku_name and cloud
merged = pd.merge(
    merged,
    billing_list_prices_df,
    left_on=['sku_name', 'cloud'],
    right_on=['sku_name', 'cloud'],
    how='left'
)

# Optionally, filter for relevant time ranges
# For example, only keep records where usage_date is within the pipeline update period
merged = merged[
    (merged['usage_date'] >= merged['period_start_time']) &
    (merged['usage_date'] <= merged['period_end_time'])
]

# Calculate estimated cost
merged['estimated_cost'] = merged['usage_quantity'] * merged['pricing.default']

# Select relevant columns for analysis
result = merged[[
    'workspace_id', 'pipeline_id', 'update_id', 'period_start_time', 'period_end_time',
    'result_state', 'usage_date', 'sku_name', 'usage_quantity', 'pricing.default', 'estimated_cost'
]]

# Display the result
print(result)
```
This code ensures efficient joins, filters, and calculations, following Databricks best practices for pandas DataFrame operations. Adjust column names as needed for your actual DataFrames.


To join the `system.lakeflow.pipeline_update_timeline` metadata with billing usage, billing list prices, and compute metadata (`node_types`, `clusters`, and `node_timeline`), you need to use the relevant keys and time ranges that connect these system tables. Here’s a breakdown and example SQL to clarify the process:

### 1. **Pipeline Update Timeline Metadata**
- Table: `system.lakeflow.pipeline_update_timeline`
- Key columns: `workspace_id`, `pipeline_id`, `update_id`
- Contains: Pipeline update events, user info, trigger type, result state, compute details[[1]](https://docs.databricks.com/aws/en/admin/system-tables/jobs/ "/docs.databricks.com/aws/en/admin/system-tables/jobs/").

### 2. **Billing Usage and List Prices**
- Table: `system.billing.usage`
  - Key columns: `workspace_id`, `usage_metadata.dlt_pipeline_id` (for DLT pipelines), `sku_name`, `usage_start_time`, `usage_end_time`
- Table: `system.billing.list_prices`
  - Key columns: `sku_name`, `cloud`, `price_start_time`, `price_end_time`
- Contains: Usage records and pricing for compute resources.

### 3. **Compute Metadata**
- Table: `system.compute.node_timeline`
  - Key columns: `workspace_id`, `cluster_id`, `instance_id`, `start_time`, `end_time`
  - Contains: Node-level resource utilization (CPU, memory, network, disk) per minute[[2]](https://docs.databricks.com/aws/en/admin/system-tables/compute/ "/docs.databricks.com/aws/en/admin/system-tables/compute/").
- Table: `system.compute.clusters`
  - Key columns: `cluster_id`, `workspace_id`
  - Contains: Cluster configuration and ownership.
- Table: `system.compute.node_types`
  - Key columns: `node_type`
  - Contains: Node type details.

---

## **Example SQL Join**

Below is an example SQL query that demonstrates how to join these tables to analyze pipeline updates, associated billing, pricing, and compute resource utilization:

```sql
SELECT
  p.workspace_id,
  p.pipeline_id,
  p.update_id,
  p.period_start_time,
  p.period_end_time,
  p.result_state,
  p.run_as_user_name,
  u.usage_date,
  u.sku_name,
  u.usage_quantity,
  u.billing_origin_product,
  lp.pricing.default AS list_price,
  u.usage_quantity * lp.pricing.default AS estimated_cost,
  nt.cluster_id,
  nt.instance_id,
  nt.start_time,
  nt.end_time,
  nt.cpu_user_percent,
  nt.mem_used_percent,
  nt.node_type
FROM system.lakeflow.pipeline_update_timeline p
LEFT JOIN system.billing.usage u
  ON p.workspace_id = u.workspace_id
  AND p.pipeline_id = u.usage_metadata.dlt_pipeline_id
  AND u.billing_origin_product = 'DLT'
  AND u.usage_date BETWEEN to_date(p.period_start_time) AND to_date(p.period_end_time)
LEFT JOIN system.billing.list_prices lp
  ON u.sku_name = lp.sku_name
  AND u.cloud = lp.cloud
  AND u.usage_date >= lp.price_start_time
  AND (lp.price_end_time IS NULL OR u.usage_date <= lp.price_end_time)
LEFT JOIN system.compute.node_timeline nt
  ON p.workspace_id = nt.workspace_id
  AND nt.start_time >= p.period_start_time
  AND nt.end_time <= p.period_end_time
-- Optionally join with clusters and node_types for more details
-- LEFT JOIN system.compute.clusters c ON nt.cluster_id = c.cluster_id
-- LEFT JOIN system.compute.node_types ntype ON nt.node_type = ntype.node_type
WHERE p.period_start_time > CURRENT_TIMESTAMP() - INTERVAL 30 DAYS
```
[[2]](https://docs.databricks.com/aws/en/admin/system-tables/compute/ "/docs.databricks.com/aws/en/admin/system-tables/compute/")[[1]](https://docs.databricks.com/aws/en/admin/system-tables/jobs/ "/docs.databricks.com/aws/en/admin/system-tables/jobs/")

---

### **Clarification**
- **Keys:** The join uses `workspace_id` and `pipeline_id` for pipeline updates and billing usage, and `cluster_id` for compute metadata.
- **Time Range:** The join ensures compute and billing records are within the pipeline update period.
- **Node Utilization:** `node_timeline` provides CPU, memory, and network stats for the compute resources used by the pipeline.
- **Pricing:** `list_prices` gives the default price for each SKU, allowing cost calculation.

---

### **Summary**
This join enables you to analyze each pipeline update with its associated billing usage, pricing, and detailed compute resource utilization, providing a comprehensive view of operational and financial metrics for declarative pipelines in Databricks[[2]](https://docs.databricks.com/aws/en/admin/system-tables/compute/ "/docs.databricks.com/aws/en/admin/system-tables/compute/")[[1]](https://docs.databricks.com/aws/en/admin/system-tables/jobs/ "/docs.databricks.com/aws/en/admin/system-tables/jobs/").

Here’s how you can join these DataFrames in Python using pandas. Assume you have the following DataFrames:

- `pipeline_update_timeline_df` (from `system.lakeflow.pipeline_update_timeline`)
- `billing_usage_df` (from `system.billing.usage`)
- `billing_list_prices_df` (from `system.billing.list_prices`)
- `node_timeline_df` (from `system.compute.node_timeline`)
- `clusters_df` (from `system.compute.clusters`)
- `node_types_df` (from `system.compute.node_types`)

Below is an example of how to perform the joins:

```python
import pandas as pd

# Join pipeline_update_timeline with billing_usage
merged1 = pd.merge(
    pipeline_update_timeline_df,
    billing_usage_df,
    left_on=['workspace_id', 'pipeline_id'],
    right_on=['workspace_id', 'usage_metadata.dlt_pipeline_id'],
    how='left'
)

# Join with billing_list_prices
merged2 = pd.merge(
    merged1,
    billing_list_prices_df,
    left_on=['sku_name', 'cloud'],
    right_on=['sku_name', 'cloud'],
    how='left'
)

# Join with node_timeline
merged3 = pd.merge(
    merged2,
    node_timeline_df,
    left_on=['workspace_id'],
    right_on=['workspace_id'],
    how='left'
)

# Join with clusters
merged4 = pd.merge(
    merged3,
    clusters_df,
    left_on=['cluster_id'],
    right_on=['cluster_id'],
    how='left'
)

# Join with node_types
final_merged = pd.merge(
    merged4,
    node_types_df,
    left_on=['node_type'],
    right_on=['node_type'],
    how='left'
)

# Now final_merged contains all joined metadata
```
This code assumes the relevant columns exist in your DataFrames. You may need to adjust column names and add time range filters as needed for your use case. For example, you can filter rows based on `period_start_time` and `period_end_time` before joining with `node_timeline_df` to ensure only relevant compute records are included.

Let me know if you want a more detailed example with sample data or additional filtering!


Here is optimized pandas code for joining the DataFrames as described, following Databricks engineering and best practices:

- Use explicit column selection to minimize memory usage.
- Filter early to reduce join size.
- Use inner joins where possible for performance.
- Calculate cost after joining billing usage and list prices.
- Align join keys with Databricks system table conventions.

```python
import pandas as pd

# Filter billing usage for DLT pipelines before joining
billing_usage_dlt = billing_usage_df[
    billing_usage_df['billing_origin_product'] == 'DLT'
]

# Join pipeline_update_timeline with billing_usage on workspace_id and pipeline_id
merged = pd.merge(
    pipeline_update_timeline_df,
    billing_usage_dlt,
    left_on=['workspace_id', 'pipeline_id'],
    right_on=['workspace_id', 'usage_metadata.dlt_pipeline_id'],
    how='inner'
)

# Join with billing_list_prices on sku_name and cloud
merged = pd.merge(
    merged,
    billing_list_prices_df,
    left_on=['sku_name', 'cloud'],
    right_on=['sku_name', 'cloud'],
    how='left'
)

# Filter node_timeline for relevant time range before joining
node_timeline_filtered = node_timeline_df[
    (node_timeline_df['start_time'] >= merged['period_start_time'].min()) &
    (node_timeline_df['end_time'] <= merged['period_end_time'].max())
]

# Join with node_timeline on workspace_id
merged = pd.merge(
    merged,
    node_timeline_filtered,
    left_on=['workspace_id'],
    right_on=['workspace_id'],
    how='left'
)

# Join with clusters on cluster_id
merged = pd.merge(
    merged,
    clusters_df,
    left_on=['cluster_id'],
    right_on=['cluster_id'],
    how='left'
)

# Join with node_types on node_type
merged = pd.merge(
    merged,
    node_types_df,
    left_on=['node_type'],
    right_on=['node_type'],
    how='left'
)

# Calculate estimated cost
merged['estimated_cost'] = merged['usage_quantity'] * merged['pricing.default']

# Select relevant columns for analysis
result = merged[[
    'workspace_id', 'pipeline_id', 'update_id', 'period_start_time', 'period_end_time',
    'result_state', 'usage_date', 'sku_name', 'usage_quantity', 'pricing.default', 'estimated_cost',
    'cluster_id', 'instance_id', 'start_time', 'end_time', 'cpu_user_percent', 'mem_used_percent', 'node_type'
]]

# Display the result
print(result)
```
[[1]](https://docs.databricks.com/aws/en/ldp/developer/python-dev/ "/docs.databricks.com/aws/en/ldp/developer/python-dev/")[[2]](https://docs.databricks.com/aws/en/ldp/developer/python-ref/ "/docs.databricks.com/aws/en/ldp/developer/python-ref/")


Here are best practices for effective communication with Claude:

**Be Explicit with Instructions**

Claude 4.x models respond well to clear, explicit instructions.[(1)](https://platform.claude.com/docs/en/build-with-claude/prompt-engineering/claude-4-best-practices#guidance-for-specific-situations)
Being specific about your desired output helps enhance results.[(1)](https://platform.claude.com/docs/en/build-with-claude/prompt-engineering/claude-4-best-practices#guidance-for-specific-situations) 
If you want comprehensive behavior, request it directly—for example: 
"Create an analytics dashboard. Include as many relevant features and interactions as possible. Go beyond the basics to 
create a fully-featured implementation." [(1)](https://platform.claude.com/docs/en/build-with-claude/prompt-engineering/claude-4-best-practices#guidance-for-specific-situations)

**Provide Context and Motivation**

Adding context or explaining why certain behavior matters helps Claude better understand your goals and 
deliver more targeted responses.[(1)](https://platform.claude.com/docs/en/build-with-claude/prompt-engineering/claude-4-best-practices#guidance-for-specific-situations) For instance, instead of "NEVER use ellipses," explain: "Your response will be read aloud by a text-to-speech engine, so never use ellipses since the text-to-speech engine will not know how to pronounce them."[(1)](https://platform.claude.com/docs/en/build-with-claude/prompt-engineering/claude-4-best-practices#guidance-for-specific-situations)

**Provide Specific Context in Prompts**

The more precise your instructions, the fewer corrections you'll need.[(2)](https://code.claude.com/docs/en/best-practices#communicate-effectively) 
Reference specific files, mention constraints, and point to example patterns.[(2)](https://code.claude.com/docs/en/best-practices#communicate-effectively) 
For example, instead of "add tests for foo.py," try "write a test for foo.py covering the edge case where the user is logged out. avoid mocks."[(2)](https://code.claude.com/docs/en/best-practices#communicate-effectively)

**Use Rich Content**

Use `@` to reference files instead of describing where code lives.[(2)](https://code.claude.com/docs/en/best-practices#communicate-effectively) 
Paste images directly by copying or dragging them into the prompt.[(2)](https://code.claude.com/docs/en/best-practices#communicate-effectively) 
Give URLs for documentation and API references.[(2)](https://code.claude.com/docs/en/best-practices#communicate-effectively) 
Pipe in data by running commands like `cat error.log | claude` to send file contents directly.[(2)](https://code.claude.com/docs/en/best-practices#communicate-effectively)

**Ask Codebase Questions**

Ask Claude questions you'd ask a senior engineer.[(2)](https://code.claude.com/docs/en/best-practices#communicate-effectively) 
When onboarding to a new codebase, use Claude for learning and exploration.[(2)](https://code.claude.com/docs/en/best-practices#communicate-effectively) 
You can ask questions like "How does logging work?" or "What edge cases does `CustomerOnboardingFlowImpl` handle?"[(2)](https://code.claude.com/docs/en/best-practices#communicate-effectively)

**Let Claude Interview You**

For larger features, have Claude interview you first.[(2)](https://code.claude.com/docs/en/best-practices#communicate-effectively)
Start with a minimal prompt and ask Claude to interview you using the `AskUserQuestion` tool.[(2)](https://code.claude.com/docs/en/best-practices#communicate-effectively)
Claude asks about things you might not have considered yet, including technical implementation, UI/UX, edge cases, and tradeoffs.[(2)](https://code.claude.com/docs/en/best-practices#communicate-effectively)

**Course-Correct Early and Often**

The best results come from tight feedback loops.[(2)](https://code.claude.com/docs/en/best-practices#communicate-effectively)
Correct Claude as soon as you notice it going off track.[(2)](https://code.claude.com/docs/en/best-practices#communicate-effectively)
Stop Claude mid-action with the `Esc` key.[(2)](https://code.claude.com/docs/en/best-practices#communicate-effectively) 
Press `Esc` twice or run `/rewind` to restore previous conversation and code state.[(2)](https://code.claude.com/docs/en/best-practices#communicate-effectively)

**Manage Context Aggressively**

Run `/clear` between unrelated tasks to reset context.[(2)](https://code.claude.com/docs/en/best-practices#communicate-effectively) 
If you've corrected Claude more than twice on the same issue in one session, the context is cluttered with failed approaches.[(2)](https://code.claude.com/docs/en/best-practices#communicate-effectively) 
A clean session with a better prompt almost always outperforms a long session with accumulated corrections.[(2)](https://code.claude.com/docs/en/best-practices#communicate-effectively)

**Be Vigilant with Examples and Details**

Claude 4.x models pay close attention to details and examples as part of their precise instruction 
following capabilities.[(1)](https://platform.claude.com/docs/en/build-with-claude/prompt-engineering/claude-4-best-practices#guidance-for-specific-situations) 
Ensure that your examples align with the behaviors you want to encourage and minimize behaviors you want to avoid.[(1)](https://platform.claude.com/docs/en/build-with-claude/prompt-engineering/claude-4-best-practices#guidance-for-specific-situations)

Prompting best practices
General principles

Examples
Be explicit with instructions
More effective:
Create an analytics dashboard. Include as many relevant features and interactions as possible. 
Go beyond the basics to create a fully-featured implementation.

Add context to improve performance
Providing context or motivation behind instructions, such as explaining why such behavior is important, can help 
Claude 4.x models better understand the goals and deliver more targeted responses.
Less effective:
NEVER use ellipses

More effective:
Your response will be read aloud by a text-to-speech engine, so never use ellipses since the text-to-speech engine
will not know how to pronounce them.

Be vigilant with examples & details
Claude 4.x models pay close attention to details and examples as part of their precise instruction following
capabilities. Ensure that your examples align with the behaviors you want to encourage and minimize behaviors you 
want to avoid.

Long-horizon reasoning and state tracking
Claude 4.5 models excel at long-horizon reasoning tasks with exceptional state tracking capabilities. 
It maintains orientation across extended sessions by focusing on incremental progress—making steady advances 
on a few things at a time rather than attempting everything at once. This capability especially emerges over 
multiple context windows or task iterations, where Claude can work on a complex task, save the state, and continue 
with a fresh context window.

Encourage complete usage of context: Prompt Claude to efficiently complete components before moving on:

Sample prompt
This is a very long task, so it may be beneficial to plan out your work clearly. 
It's encouraged to spend your entire output context working on the task - just make sure you don't run out of context 
with significant uncommitted work. Continue working systematically until you have completed this task.

Communication style
Claude 4.5 models have a more concise and natural communication style compared to previous models:
More direct and grounded: Provides fact-based progress reports rather than self-celebratory updates
More conversational: Slightly more fluent and colloquial, less machine-like
Less verbose: May skip detailed summaries for efficiency unless prompted otherwise
This communication style accurately reflects what has been accomplished without unnecessary elaboration.

Guidance for specific situations
Balance verbosity
Claude 4.5 models tend toward efficiency and may skip verbal summaries after tool calls, jumping directly 
to the next action. While this creates a streamlined workflow, you may prefer more visibility into its reasoning process.

If you want Claude to provide updates as it works:

Sample prompt
After completing a task that involves tool use, provide a quick summary of the work you've done.

To make Claude more proactive about taking action by default, you can add this to the prompt:

Sample prompt for proactive action
<default_to_action>
By default, implement changes rather than only suggesting them. If the user's intent is unclear, infer the most useful 
likely action and proceed, using tools to discover any missing details instead of guessing. 
Try to infer the user's intent about whether a tool call (e.g., file edit or read) is intended or not, and act accordingly.
</default_to_action>

On the other hand, if you want the model to be more hesitant by default, less prone to jumping straight into implementations, 
and only take action if requested, you can steer this behavior with a prompt like the below:

Sample prompt for conservative action
<do_not_act_before_instructions>
Do not jump into implementatation or changes files unless clearly instructed to make changes. When the user's intent is ambiguous, default to providing information, doing research, and providing recommendations rather than taking action. Only proceed with edits, modifications, or implementations when the user explicitly requests them.
</do_not_act_before_instructions>

Control the format of responses
There are a few ways that we have found to be particularly effective in steering output formatting in Claude 4.x models:

1. Tell Claude what to do instead of what not to do

Instead of: "Do not use markdown in your response"
Try: "Your response should be composed of smoothly flowing prose paragraphs."

2. Use XML format indicators

Try: "Write the prose sections of your response in <smoothly_flowing_prose_paragraphs> tags."

3. Match your prompt style to the desired output

The formatting style used in your prompt may influence Claude's response style. If you are still experiencing 
steerability issues with output formatting, we recommend as best as you can matching your prompt style 
to your desired output style. For example, removing markdown from your prompt can reduce the volume of markdown 
in the output.

4. Use detailed prompts for specific formatting preferences

For more control over markdown and formatting usage, provide explicit guidance:

Sample prompt to minimize markdown
<avoid_excessive_markdown_and_bullet_points>
When writing reports, documents, technical explanations, analyses, or any long-form content, write in clear, 
flowing prose using complete paragraphs and sentences. Use standard paragraph breaks for organization and 
reserve markdown primarily for `inline code`, code blocks (```...```), and simple headings (###, and ###). 
Avoid using **bold** and *italics*.

DO NOT use ordered lists (1. ...) or unordered lists (*) unless : a) you're presenting truly discrete 
items where a list format is the best option, or b) the user explicitly requests a list or ranking.

Instead of listing items with bullets or numbers, incorporate them naturally into sentences. 
This guidance applies especially to technical writing. Using prose instead of excessive formatting 
will improve user satisfaction. NEVER output a series of overly short bullet points.

Your goal is readable, flowing text that guides the reader naturally through ideas rather than fragmenting 
information into isolated points.
</avoid_excessive_markdown_and_bullet_points>

Research and information gathering
Claude 4.5 models demonstrate exceptional agentic search capabilities and can find and synthesize information 
from multiple sources effectively. For optimal research results:

1. Provide clear success criteria: Define what constitutes a successful answer to your research question
2. Encourage source verification: Ask Claude to verify information across multiple sources
3. For complex research tasks, use a structured approach:

Sample prompt for complex research
Search for this information in a structured way. As you gather data, develop several competing hypotheses. 
Track your confidence levels in your progress notes to improve calibration. Regularly self-critique your approach and 
plan. Update a hypothesis tree or research notes file to persist information and provide transparency. 
Break down this complex research task systematically.

This structured approach allows Claude to find and synthesize virtually any piece of information and 
iteratively critique its findings, no matter the size of the corpus.


Document creation
Claude 4.5 models excel at creating presentations, animations, and visual documents. These models match or exceed 
Claude Opus 4.1 in this domain, with impressive creative flair and stronger instruction following. 
The models produce polished, usable output on the first try in most cases.

For best results with document creation:

Sample prompt
Create a professional presentation on [topic]. Include thoughtful design elements, visual hierarchy, and 
engaging animations where appropriate.

Reduce file creation in agentic coding
Claude 4.x models may sometimes create new files for testing and iteration purposes, particularly when working with code.
This approach allows Claude to use files, especially python scripts, as a 'temporary scratchpad' before saving its 
final output. Using temporary files can improve outcomes particularly for agentic coding use cases.

If you'd prefer to minimize net new file creation, you can instruct Claude to clean up after itself:

Sample prompt
If you create any temporary new files, scripts, or helper files for iteration, clean up these files by removing them 
at the end of the task.

Overeagerness and file creation
Claude Opus 4.5 has a tendency to overengineer by creating extra files, adding unnecessary abstractions, 
or building in flexibility that wasn't requested. If you're seeing this undesired behavior, add explicit prompting 
to keep solutions minimal.

For example:

Sample prompt to minimize overengineering
Avoid over-engineering. Only make changes that are directly requested or clearly necessary. 
Keep solutions simple and focused.

Don't add features, refactor code, or make "improvements" beyond what was asked. 
A bug fix doesn't need surrounding code cleaned up. 
A simple feature doesn't need extra configurability.

Don't add error handling, fallbacks, or validation for scenarios that can't happen. Trust internal code and 
framework guarantees. Only validate at system boundaries (user input, external APIs). 
Don't use backwards-compatibility shims when you can just change the code.

Don't create helpers, utilities, or abstractions for one-time operations. 
Don't design for hypothetical future requirements. 
The right amount of complexity is the minimum needed for the current task. 
Reuse existing abstractions where possible and follow the DRY principle.

Avoid focusing on passing tests and hard-coding
Claude 4.x models can sometimes focus too heavily on making tests pass at the expense of more general solutions, 
or may use workarounds like helper scripts for complex refactoring instead of using standard tools directly. 
To prevent this behavior and ensure robust, generalizable solutions:

Sample prompt
Please write a high-quality, general-purpose solution using the standard tools available. 
Do not create helper scripts or workarounds to accomplish the task more efficiently. 
Implement a solution that works correctly for all valid inputs, not just the test cases. 
Do not hard-code values or create solutions that only work for specific test inputs. 
Instead, implement the actual logic that solves the problem generally.

Focus on understanding the problem requirements and implementing the correct algorithm. 
Tests are there to verify correctness, not to define the solution. 
Provide a principled implementation that follows best practices and software design principles.

If the task is unreasonable or infeasible, or if any of the tests are incorrect, please inform me rather than working around them.
The solution should be robust, maintainable, and extendable.

Encouraging code exploration
Claude Opus 4.5 is highly capable but can be overly conservative when exploring code. 
If you notice the model proposing solutions without looking at the code or 
making assumptions about code it hasn't read, the best solution is to add explicit instructions to the prompt. 
Claude Opus 4.5 is our most steerable model to date and responds reliably to direct guidance.

For example:

Sample prompt for code exploration
ALWAYS read and understand relevant files before proposing code edits. 
Do not speculate about code you have not inspected. 
If the user references a specific file/path, you MUST open and inspect it before explaining or proposing fixes. 
Be rigorous and persistent in searching code for key facts. 
Thoroughly review the style, conventions, and abstractions of the codebase before implementing new features or abstractions.

Minimizing hallucinations in agentic coding
Claude 4.x models are less prone to hallucinations and give more accurate, grounded, intelligent answers
based on the code. To encourage this behavior even more and minimize hallucinations:

Sample prompt
<investigate_before_answering>
Never speculate about code you have not opened. 
If the user references a specific file, you MUST read the file before answering. 
Make sure to investigate and read relevant files BEFORE answering questions about the codebase. 
Never make any claims about code before investigating unless you are certain of the correct answer - give grounded
and hallucination-free answers.
</investigate_before_answering>

Considerations
Be specific about desired behavior: Consider describing exactly what you'd like to see in the output.
Frame your instructions with modifiers: Adding modifiers that encourage Claude to increase the quality 
and detail of its output can help better shape Claude's performance. 
For example, instead of "Create an analytics dashboard", use "Create an analytics dashboard. Include as many relevant features and interactions as possible. Go beyond the basics to create a fully-featured implementation."
Request specific features explicitly: Animations and interactive elements should be requested explicitly when desired.

Quick Start for Coding 

Step 1. Ask question 
Let’s start with understanding your codebase. Try one of these commands:

what does this project do?

Claude will analyze your files and provide a summary. You can also ask more specific questions:

what technologies does this project use?

where is the main entry point?

explain the folder structure

You can also ask Claude about its own capabilities:

what can Claude Code do?

how do I create custom skills in Claude Code?

can Claude Code work with Docker?

Claude Code reads your files as needed - you don’t have to manually add context. 
Claude also has access to its own documentation and can answer questions about its features and capabilities.
​
Step 2. Make code change:

Now let’s make Claude Code do some actual coding. Try a simple task:

add a hello world function to the main file

Claude Code will:
Find the appropriate file
Show you the proposed changes
Ask for your approval
Make the edit

Step 3. Use Git with Claude Code

Claude Code makes Git operations conversational:
what files have I changed?
commit my changes with a descriptive message
You can also prompt for more complex Git operations:
create a new branch called feature/quickstart
show me the last 5 commits
help me resolve merge conflicts
​
Step 4: Fix a bug or add a feature

Claude is proficient at debugging and feature implementation.

Describe what you want in natural language:
add input validation to the user registration form

Or fix existing issues:
there's a bug where users can submit empty forms - fix it

Claude Code will:
Locate the relevant code
Understand the context
Implement a solution
Run tests if available
​
Step 5: Test out other common workflows
There are a number of ways to work with Claude:

Refactor code
refactor the authentication module to use async/await instead of callbacks

Write tests
write unit tests for the calculator functions

Update documentation
update the README with installation instructions

Code review
review my changes and suggest improvements

Remember: Claude Code is your AI pair programmer. Talk to it like you would a helpful colleague - describe 
what you want to achieve, and it will help you get there.
​
​
Pro tips

Be specific with your requests
Instead of: “fix the bug”
Try: “fix the login bug where users see a blank screen after entering wrong credentials”

Use step-by-step instructions
Break complex tasks into steps:
1. create a new database table for user profiles
2. create an API endpoint to get and update user profiles
3. build a webpage that allows users to see and edit their information

Let Claude explore first

Before making changes, let Claude understand your code:

analyze the database schema

build a dashboard showing products that are most frequently returned by our UK customers

Save time with shortcuts
Press ? to see all available keyboard shortcuts
Use Tab for command completion
Press ↑ for command history
Type / to see all commands and skills​

What’s next?

How Claude Code works
Understand the agentic loop, built-in tools, and how Claude Code interacts with your project
https://code.claude.com/docs/en/how-claude-code-works

Best practices
Get better results with effective prompting and project setup
https://code.claude.com/docs/en/best-practices

Common workflows
Step-by-step guides for common tasks
https://code.claude.com/docs/en/common-workflows

Extend Claude Code
Customize with CLAUDE.md, skills, hooks, MCP, and more
https://code.claude.com/docs/en/features-overview​




Claude Code
Explore first, then plan, then code

See https://code.claude.com/docs/en/best-practices

The recommended workflow has four phases:

Explore

read /src/auth and understand how we handle sessions and login.
also look at how we manage environment variables for secrets.

# One-off queries
claude -p "Explain what this project does"

# Structured output for scripts
claude -p "List all API endpoints" --output-format json

# Streaming for real-time processing
claude -p "Analyze this log file" --output-format stream-json

Plan
Plan Mode is useful, but also adds overhead.
For tasks where the scope is clear and the fix is small (like fixing a typo, adding a log line, or renaming a variable) ask Claude to do it directly.
Planning is most useful when you’re uncertain about the approach, when the change modifies multiple files, or when you’re unfamiliar 
with the code being modified. If you could describe the diff in one sentence, skip the plan.


Ask to create a detailed implementation plan.

I want to add OAuth. What files need to change?
What's the session flow? Create a plan.

For example, use a Writer/Reviewer pattern:
Session A (Writer)
Implement a rate limiter for our API endpoints	

Session B (Reviewer)
Review the rate limiter implementation in @src/middleware/rateLimiter.ts. Look for edge cases, race conditions, 
and consistency with our existing middleware patterns.

Session C (Writer)
Here's the review feedback: [Session B output]. Address these issues.

Implement

let code, verifying against its plan.

implement the OAuth flow from the plan. write tests for the
callback handler, run the test suite and fix any failures.

Commit

commit with a descriptive message and create a PR.

Provide specific context in your prompts

Vague prompts can be useful when you’re exploring and can afford to course-correct. A prompt like "what would you improve in this file?" can surface 
things you wouldn’t have thought to ask about.

The more precise instructions, the fewer corrections need.

Claude can infer intent, but it can’t read mind. Reference specific files, mention constraints, and point to example patterns.

Strategy:

Scope the task. Specify which file, what scenario, and testing preferences.
"write a test for foo.py covering the edge case where the user is logged out. avoid mocks.”

Point to sources. Direct Claude to the source that can answer a question
"look through ExecutionFactory’s git history and summarize how its api came to be”

Reference existing patterns. Point Claude to patterns in your codebase.
"look at how existing widgets are implemented on the home page to understand the patterns. HotDogWidget.php is a good example. 
follow the pattern to implement a new calendar widget that lets the user select a month and paginate forwards/backwards to pick a year. 
build from scratch without libraries other than the ones already used in the codebase.”

Describe the symptom. Provide the symptom, the likely location, and what “fixed” looks like.
"users report that login fails after session timeout. check the auth flow in src/auth/, especially token refresh. 
write a failing test that reproduces the issue, then fix it”


Provide rich content
Use @ to reference files, paste screenshots/images, or pipe data directly.

You can provide rich data to Claude in several ways:
Reference files with @ instead of describing where code lives. Claude reads the file before responding.
Paste images directly. Copy/paste or drag and drop images into the prompt.
Give URLs for documentation and API references. Use /permissions to allowlist frequently-used domains.
Pipe in data by running cat error.log | claude to send file contents directly.
Let Claude fetch what it needs. Tell Claude to pull context itself using Bash commands, MCP tools, or by reading files.

Set up Environment 
Run /init to generate a starter CLAUDE.md file based on your current project structure, then refine over time.

CLAUDE.md is a special file that Claude reads at the start of every conversation. Include Bash commands, code style,
and workflow rules. This gives Claude persistent context it can’t infer from code alone.

The /init command analyzes your codebase to detect build systems, test frameworks, and code patterns, giving you a solid foundation to refine.
There’s no required format for CLAUDE.md files, but keep it short and human-readable. For example:

# Code style
- Use ES modules (import/export) syntax, not CommonJS (require)
- Destructure imports when possible (eg. import { foo } from 'bar')

# Workflow
- Be sure to typecheck when you're done making a series of code changes
- Prefer running single tests, and not the whole test suite, for performance

CLAUDE.md is loaded every session, so only include things that apply broadly. For domain knowledge or workflows that are 
only relevant sometimes, use skills instead. Claude loads them on demand without bloating every conversation.
Keep it concise. For each line, ask: “Would removing this cause Claude to make mistakes?” If not, cut it. 
Bloated CLAUDE.md files cause Claude to ignore your actual instructions!

✅ Include	                                               ❌ Exclude
Bash commands Claude can’t guess	                           Anything Claude can figure out by reading code
Code style rules that differ from defaults	               Standard language conventions Claude already knows
Testing instructions and preferred test runners	            Detailed API documentation (link to docs instead)
Repository etiquette (branch naming, PR conventions)	      Information that changes frequently
Architectural decisions specific to your project	         Long explanations or tutorials
Developer environment quirks (required env vars)	         File-by-file descriptions of the codebase
Common gotchas or non-obvious behaviors	                  Self-evident practices like “write clean code”


If Claude keeps doing something you don’t want despite having a rule against it, the file is probably too 
long and the rule is getting lost. If Claude asks you questions that are answered in CLAUDE.md, the phrasing might be ambiguous. 
Treat CLAUDE.md like code: review it when things go wrong, prune it regularly, and test changes by observing whether Claude’s behavior actually shifts.

You can tune instructions by adding emphasis (e.g., “IMPORTANT” or “YOU MUST”) to improve adherence. Check CLAUDE.md into git so your team can contribute. 
The file compounds in value over time.
CLAUDE.md files can import additional files using @path/to/import syntax:

See @README.md for project overview and @package.json for available npm commands.

# Additional Instructions
- Git workflow: @docs/git-instructions.md
- Personal overrides: @~/.claude/my-project-instructions.md

You can place CLAUDE.md files in several locations:
Home folder (~/.claude/CLAUDE.md): Applies to all Claude sessions
Project root (./CLAUDE.md): Check into git to share with your team, or name it CLAUDE.local.md and .gitignore it
Parent directories: Useful for monorepos where both root/CLAUDE.md and root/foo/CLAUDE.md are pulled in automatically
Child directories: Claude pulls in child CLAUDE.md files on demand when working with files in those directories

Create Skills

Create SKILL.md files in .claude/skills/ to give Claude domain knowledge and reusable workflows.
Skills extend Claude’s knowledge with information specific to your project, team, or domain. 
Claude applies them automatically when relevant, or you can invoke them directly with /skill-name.

Create a skill by adding a directory with a SKILL.md to .claude/skills/:

---
name: api-conventions
description: REST API design conventions for our services
---
# API Conventions
- Use kebab-case for URL paths
- Use camelCase for JSON properties
- Always include pagination for list endpoints
- Version APIs in the URL path (/v1/, /v2/)

Skills can also define repeatable workflows you invoke directly:
---
name: fix-issue
description: Fix a GitHub issue
disable-model-invocation: true
---
Analyze and fix the GitHub issue: $ARGUMENTS.

1. Use `gh issue view` to get the issue details
2. Understand the problem described in the issue
3. Search the codebase for relevant files
4. Implement the necessary changes to fix the issue
5. Write and run tests to verify the fix
6. Ensure code passes linting and type checking
7. Create a descriptive commit message
8. Push and create a PR


Commuicate Effectively 
The way you communicate with Claude Code significantly impacts the quality of results.

Ask codebase questions

Ask Claude questions you’d ask a senior engineer.

When onboarding to a new codebase, use Claude Code for learning and exploration. You can ask Claude the same sorts of questions 
you would ask another engineer:

How does logging work?
How do I make a new API endpoint?
What does async move { ... } do on line 134 of foo.rs?
What edge cases does CustomerOnboardingFlowImpl handle?
Why does this code call foo() instead of bar() on line 333?

Using Claude Code this way is an effective onboarding workflow, improving ramp-up time and reducing load on other engineers. 
No special prompting required: ask questions directly.


Let Claude interview you
For larger features, have Claude interview you first. Start with a minimal prompt and ask Claude to interview you using the AskUserQuestion tool.

Claude asks about things you might not have considered yet, including technical implementation, UI/UX, edge cases, and tradeoffs.

I want to build [brief description]. Interview me in detail using the AskUserQuestion tool.

Ask about technical implementation, UI/UX, edge cases, concerns, and tradeoffs. Don't ask obvious questions, dig into the hard parts I might not have considered.

Keep interviewing until we've covered everything, then write a complete spec to SPEC.md.

Once the spec is complete, start a fresh session to execute it. 
The new session has clean context focused entirely on implementation, and you have a written spec to reference.

Manage your session
Conversations are persistent and reversible. Use this to your advantage!

Course-correct early and often
Correct Claude as soon as you notice it going off track.

The best results come from tight feedback loops. Though Claude occasionally solves problems perfectly on the first attempt, 
correcting it quickly generally produces better solutions faster.

If you’ve corrected Claude more than twice on the same issue in one session, the context is cluttered with failed approaches. Run /clear 
and start fresh with a more specific prompt that incorporates what you learned. 
A clean session with a better prompt almost always outperforms a long session with accumulated corrections.


Manage context aggressively

Run /clear between unrelated tasks to reset context.

Claude Code automatically compacts conversation history when you approach context limits, which preserves important code and decisions while freeing space.

During long sessions, Claude’s context window can fill with irrelevant conversation, file contents, and commands. This can reduce performance and sometimes distract Claude.

Use /clear frequently between tasks to reset the context window entirely

When auto compaction triggers, Claude summarizes what matters most, including code patterns, file states, and key decisions

For more control, run /compact <instructions>, like /compact Focus on the API changes
Customize compaction behavior in CLAUDE.md with instructions like "When compacting, always preserve the full list of modified files 
and any test commands" to ensure critical context survives summarization


Avoid common failure patterns

These are common mistakes. Recognizing them early saves time:
The kitchen sink session. You start with one task, then ask Claude something unrelated, then go back to the first task. Context is full of irrelevant information.
Fix: /clear between unrelated tasks.
Correcting over and over. Claude does something wrong, you correct it, it’s still wrong, you correct again. Context is polluted with failed approaches.
Fix: After two failed corrections, /clear and write a better initial prompt incorporating what you learned.
The over-specified CLAUDE.md. If your CLAUDE.md is too long, Claude ignores half of it because important rules get lost in the noise.
Fix: Ruthlessly prune. If Claude already does something correctly without the instruction, delete it or convert it to a hook.
The trust-then-verify gap. Claude produces a plausible-looking implementation that doesn’t handle edge cases.
Fix: Always provide verification (tests, scripts, screenshots). If you can’t verify it, don’t ship it.
The infinite exploration. You ask Claude to “investigate” something without scoping it. Claude reads hundreds of files, filling the context.
Fix: Scope investigations narrowly or use subagents so the exploration doesn’t consume your main context.


Develop your intuition

The patterns in this guide aren’t set in stone. They’re starting points that work well in general, but might not be optimal for every situation.

Sometimes you should let context accumulate because you’re deep in one complex problem and the history is valuable. 
Sometimes you should skip planning and let Claude figure it out because the task is exploratory. 
Sometimes a vague prompt is exactly right because you want to see how Claude interprets the problem before constraining it.
Pay attention to what works. When Claude produces great output, notice what you did: the prompt structure, the context you provided, the mode you were in. 
When Claude struggles, ask why. Was the context too noisy? The prompt too vague? The task too big for one pass?
Over time, you’ll develop intuition that no guide can capture. You’ll know when to be specific and when to be open-ended, 
when to plan and when to explore, when to clear context and when to let it accumulate.

Related resources
How Claude Code works
Understand the agentic loop, built-in tools, and how Claude Code interacts with your project.
https://code.claude.com/docs/en/how-claude-code-works

Extend Claude Code
Choose between skills, hooks, MCP, subagents, and plugins
https://code.claude.com/docs/en/features-overview

Common workflows
Step-by-step recipes for debugging, testing, PRs, and more
https://code.claude.com/docs/en/common-workflows

CLAUDE.md
Store project conventions and persistent context
https://code.claude.com/docs/en/memory




--------------------Core Findings for Lakeflow Declarative Pipeline -------------------

The `system.lakeflow.pipeline` table in Databricks provides essential metadata about Lakeflow Spark Declarative Pipelines. 
Here’s a breakdown of what it is, why it matters, when to use it, and how to leverage its metrics—fully aligned 
with Databricks best practices:

---

**What is the `system.lakeflow.pipeline` table?**

- It is a system table that stores metadata about each Lakeflow pipeline in Databricks workspace.
- Key columns include pipeline identifiers, configuration, settings, type, creator, and execution context.
- This table does not store operational metrics (like throughput or latency), but rather the descriptive and 
configuration metadata for each pipeline.
- Tracks all pipelines created in the account.

---

**Why use metadata metrics from this table?**

- **Governance & Auditing:** Track who created pipelines, their configuration, and execution context for compliance 
and operational transparency.
- **Pipeline Management:** Quickly list, filter, and review pipeline configurations to ensure they meet organizational standards.
- **Change Tracking:** Monitor changes in pipeline settings, types, and ownership to support versioning and change management.
- **Resource Optimization:** Understand pipeline configurations to optimize cluster usage and resource allocation in line 
with best practices[[1]](https://docs.databricks.com/aws/en/data-engineering/observability-best-practices/ "/docs.databricks.com/aws/en/data-engineering/observability-best-practices/").

---

**When should use these metrics?**
- **During pipeline creation and onboarding:** Validate that new pipelines are configured correctly and follow best practices.
- **For regular audits:** Periodically review pipeline metadata to ensure compliance, security, and optimal resource usage.
- **When troubleshooting:** Use metadata to correlate pipeline configuration with operational issues or performance bottlenecks.
- **For reporting and inventory:** Generate lists of all pipelines, their owners, and configurations for management and governance.

---

**How to use the metadata metrics from `system.lakeflow.pipeline`?**

1. **Query the Table:** Use Databricks SQL to extract pipeline metadata for analysis or reporting.
   ```sql
   SELECT workspace_id, pipeline_id, name, pipeline_type, created_by, run_as, settings, configuration
   FROM system.lakeflow.pipeline
   ```
2. **Audit Ownership and Permissions:** Review `created_by` and `run_as` to ensure pipelines are managed by authorized users or service principals.
3. **Validate Configuration:** Check the `settings` and `configuration` columns to confirm pipelines are using recommended cluster sizes, libraries, and Spark configurations.
4. **Inventory Pipelines:** List all pipelines by `pipeline_type` and `name` to understand the landscape of data processing in your workspace.
5. **Monitor for Compliance:** Use metadata to ensure pipelines adhere to data governance, security, and operational standards[[2]](https://docs.databricks.com/aws/en/lakehouse-architecture/data-governance/best-practices/ "/docs.databricks.com/aws/en/lakehouse-architecture/data-governance/best-practices/").

---

**Best Practices:**

- Regularly review pipeline metadata to ensure alignment with organizational policies and resource optimization strategies.
- Use the metadata in conjunction with operational metrics (from other system tables) for holistic observability and troubleshooting[[1]](https://docs.databricks.com/aws/en/data-engineering/observability-best-practices/ "/docs.databricks.com/aws/en/data-engineering/observability-best-practices/").
- Automate reporting and alerts for unauthorized changes or non-compliant configurations.

---

By leveraging the `system.lakeflow.pipeline` table, gain visibility and control over Lakeflow pipelines, supporting governance, 
optimization, and reliable data engineering operations in Databricks.


Here’s a detailed breakdown of the key column values in the `system.lakeflow.pipelines` table, fully aligned 
with Databricks documentation:


**1. workspace_id**  
- **Description:** The unique identifier for the Databricks workspace where the pipeline resides.  
- **Value:** A string, such as `"1234567890123456"`. This helps to distinguish pipelines across different workspaces in Databricks account[[1]]

**2. pipeline_id**  
- **Description:** The unique identifier for the pipeline within a workspace.  
- **Value:** A string, such as `"abcdef1234567890"`. Note that this is only unique within a single workspace, so should always 
use it together with `workspace_id` for global uniqueness[[1]]
**3. settings**  
- **Description:** A struct containing the settings of the pipeline.  
- **Value:** This includes configuration options such as cluster settings, libraries, storage locations, and other pipeline-specific parameters.  
- **Reference:** For a full list and explanation of possible settings, see the [Pipeline settings reference](https://docs.databricks.com/aws/en/admin/system-tables/jobs#pipeline-settings-reference)
**4. pipeline_type**  
- **Description:** The type of the pipeline.  
- **Value:** A string indicating the pipeline’s type. 
- **Reference:** For all possible values and their meanings, see https://docs.databricks.com/aws/en/admin/system-tables/jobs#pipeline-type-values

**5. name**  
- **Description:** The user-supplied name of the pipeline.  
- **Value:** A string, such as `"Customer Data ETL"`. This is set by the user when creating the pipeline and is used for display and identification purposes[[1]].

**6. created_by**  
- **Description:** The service principal ID of the user who created the pipeline.  
- **Value:** A string, such as service principal ID. This is useful for auditing and tracking ownership[[1]]

**7. run_as**  
- **Description:** The service principal ID whose permissions are used for the pipeline run.  
- **Value:** A string determines the security context under which the pipeline executes[[1]]
**8. configuration**  
- **Description:** A map containing user-supplied configuration for the pipeline.  
- **Value:** Key-value pairs, such as `{"spark.sql.shuffle.partitions": "200", "custom_param": "value"}`. This allows users to pass custom Spark or pipeline parameters at runtime[[1]]
**Summary Table Example:**

| Column         | Example Value                | Description/Notes                                                                 |
|----------------|-----------------------------|-----------------------------------------------------------------------------------|
| workspace_id   | "1234567890123456"          | Unique workspace identifier                                                       |
| pipeline_id    | "abcdef1234567890"          | Unique pipeline identifier (within workspace)                                     |
| settings       | {cluster, storage, ...}     | Struct with pipeline settings (see reference for details)                         |
| pipeline_type  | "DELTA_LIVE_TABLES"         | Type of pipeline (see reference for all possible values)                          |
| name           | "Customer Data ETL"         | User-supplied pipeline name                                                       |
| created_by     | "user@example.com"          | Creator’s email or service principal ID                                           |
| run_as         | "service-principal@example.com" | Permissions context for pipeline execution                                    |
| configuration  | {"spark.sql.shuffle.partitions": "200"} | User-supplied configuration map                                      |

These columns provide essential metadata for managing, auditing, and configuring pipelines in Databricks.


The system.lakeflow.pipeline_update_timeline table in Databricks is a system table that tracks the pipeline updates, provides metadata metrics about 
Lakeflow Spark Declarative Pipeline updates. 

Here’s a breakdown of what, why, when, and how to use these metrics, fully aligned with Databricks best practices:

**What is the system.lakeflow.pipeline_update_timeline table?**  
This table records metadata about pipeline update events, such as when a pipeline update was started, completed, and its status. 
It helps to track the lifecycle and performance of pipeline updates, including scheduling, execution, and any errors or issues encountered.

**Why use metadata metrics from this table?**  
- **Observability:** It enables to monitor pipeline health, performance, and reliability by providing a historical log of pipeline update events.
- **Troubleshooting:** Identify failed updates, bottlenecks, or delays by analyzing update durations and statuses.
- **Optimization:** By reviewing update timelines, tune pipeline schedules, cluster configurations, and resource allocation 
for better cost and latency management.
- **Compliance and Auditing:** The table provides an audit trail of pipeline operations, which is useful for governance and compliance reporting.

**When should you use these metrics?**  
- **Regular Monitoring:** Incorporate these metrics into observability dashboards to continuously monitor pipeline performance 
and reliability[[1]](https://docs.databricks.com/aws/en/data-engineering/observability-best-practices/ "/docs.databricks.com/aws/en/data-engineering/observability-best-practices/").
- **After Pipeline Updates:** Review the table after running pipeline updates to verify successful completion and investigate
any failures or anomalies[[2]](https://docs.databricks.com/aws/en/ldp/develop/ "/docs.databricks.com/aws/en/ldp/develop/").
- **During Optimization Efforts:** Use the metrics when tuning pipeline schedules, cluster sizes, or troubleshooting performance issues.
- **For Auditing:** Periodically review the table for compliance and operational audits.

**How to use the metadata metrics from system.lakeflow.pipeline_update_timeline?**  
1. **Query the Table:** Use Databricks SQL or notebooks to query the table for relevant columns 
such as update start time, end time, status, and error messages.
   ```sql
   SELECT * FROM system.lakeflow.pipeline_update_timeline
   WHERE pipeline_id = '<your_pipeline_id>'
   ORDER BY update_start_time DESC;
   ```
2. **Analyze Update Durations:** Calculate the time taken for each update to identify slow or failed updates.
3. **Monitor Statuses:** Filter for failed or incomplete updates to trigger alerts or initiate troubleshooting.
4. **Integrate with Dashboards:** Visualize update timelines and statuses in Databricks dashboards for real-time monitoring.
5. **Correlate with Other Metrics:** Combine with cluster utilization, throughput, and latency metrics for holistic pipeline observability[[1]](https://docs.databricks.com/aws/en/data-engineering/observability-best-practices/ "/docs.databricks.com/aws/en/data-engineering/observability-best-practices/").

**Best Practices:**
- Regularly monitor pipeline update timelines alongside other key metrics (backpressure, throughput, duration, latency, cluster utilization) 
for comprehensive observability[[1]](https://docs.databricks.com/aws/en/data-engineering/observability-best-practices/ "/docs.databricks.com/aws/en/data-engineering/observability-best-practices/").
- Use the table to inform predictive optimization and maintenance schedules, ensuring efficient resource use 
and cost management[[2]](https://docs.databricks.com/aws/en/ldp/develop/ "/docs.databricks.com/aws/en/ldp/develop/").
- Automate alerts for failed or delayed updates to enable proactive troubleshooting.

By leveraging the system.lakeflow.pipeline_update_timeline table, ensure robust observability, efficient operations, 
and compliance for Lakeflow Spark Declarative Pipelines in Databricks.

The system.lakeflow.pipeline_update_timeline table in Databricks is a system table that tracks the pipeline updates, provides detailed metadata 
about Lakeflow pipeline updates. 
Here is a complete, accurate, and Databricks-aligned breakdown of all its columns, including explanations and clarifications for each[[1]]
**Column Name** | **Data Type** | **Description & Clarification**
--- | --- | ---
`account_id` | string | The ID of the Databricks account that owns the pipeline. Useful for multi-account environments.
`workspace_id` | string | The ID of the workspace where the pipeline resides. Ensures you can distinguish pipelines across workspaces.
`pipeline_id` | string | The unique identifier for the pipeline within a workspace. Used to filter and analyze updates for a specific pipeline.
`update_id` | string | The unique identifier for each pipeline update event within a workspace. Allows tracking of individual update attempts.
`update_type` | string | The type of pipeline update (e.g., full refresh, incremental). For possible values, refer to Databricks documentation on pipeline update types.
`request_id` | string | The ID of the request that initiated the update. Helps track retries or restarts of the same update.
`run_as_user_name` | string | The email or service principal ID whose permissions were used for the update. Useful for auditing and access tracking.
`trigger_type` | string | Indicates what triggered the update (e.g., schedule, manual, API). For possible values, see pipeline trigger type documentation.
`trigger_details` | struct | Additional details about the trigger (e.g., schedule info, user info). For possible values, see pipeline trigger type details.
`result_state` | string | The outcome of the pipeline update. For long-running updates split across multiple rows, this is only populated in the row representing the end of the update.
`compute` | struct | Details about the compute resources used for the update (e.g., cluster info, resource allocation).
`period_start_time` | timestamp | The UTC timestamp marking the start of the pipeline update or the start of the hour for long updates. Timezone info is included.
`period_end_time` | timestamp | The UTC timestamp marking the end of the pipeline update or the end of the hour for long updates. Timezone info is included.
`refresh_selection` | array | List of tables updated without a full refresh. Useful for understanding incremental update scope.
`full_refresh_selection` | array | List of tables updated with a full refresh. Useful for tracking full data reloads.
`reset_checkpoint_selection` | array | List of streaming flows for which checkpoints were cleared. Important for streaming pipeline troubleshooting.

**Key Points:**
- The table is immutable and complete at the time it is produced, ensuring reliable historical tracking.
- Time columns (`period_start_time`, `period_end_time`) are sliced hourly for long-running updates, which helps with granular monitoring and troubleshooting.
- The table supports both auditing (who ran what, when, and how) and operational monitoring (success/failure, resource usage, update types).

**Example Query:**
To get the daily pipeline update count for a workspace for the last 7 days:
```sql
SELECT
  workspace_id,
  COUNT(DISTINCT update_id) as update_count,
  to_date(period_start_time) as date
FROM system.lakeflow.pipeline_update_timeline
WHERE period_start_time > CURRENT_TIMESTAMP() - INTERVAL 7 DAYS
GROUP BY ALL
```
[[1]]

This schema enables robust observability, troubleshooting, and optimization for Lakeflow pipeline operations in Databricks.

Here is a breakdown of the columns in system.lakeflow.pipeline_update_timeline that address requirements, 
fully aligned with Databricks documentation:

1. **result_state**  
   - **Description:** The outcome of the pipeline update.  
   - **Details:** For updates running across more than 1 hour and split across multiple rows, this column is populated only in the row that represents the end of the update.  

2. **compute**  
   - **Description:** Details about the compute resource used in the pipeline update.  
   - **Details:** This is a struct containing information such as cluster configuration and resource allocation used for the update[[1]]

3. **period_start_time**  
   - **Description:** The start time for the pipeline update or for the hour (for long updates).  
   - **Details:** Stored as a UTC timestamp, with timezone information recorded at the end of the value.

4. **period_end_time**  
   - **Description:** The end time for the pipeline update or for the hour (for long updates).  
   - **Details:** Also stored as a UTC timestamp, with timezone information.

5. **trigger_type**  
   - **Description:** What triggered this update (e.g., schedule, manual, API).  
   - **Details:** For possible values, see Pipeline trigger type values in Databricks documentation[[1]].

6. **trigger_details**  
   - **Description:** The details of the pipeline's trigger.  
   - **Details:** This is a struct with additional information about the trigger, such as schedule info or user info[[1]]

7. **request_id**  
   - **Description:** The ID of the request that initiated the update.  
   - **Details:** Helps to understand how many times an update had to be retried or restarted[[1]]

8. **run_as_user_name**  
   - **Description:** The email of the user or the ID of the service principal whose permissions are used for the pipeline update.

9. **update_type**  
   - **Description:** The type of the pipeline update (e.g., full refresh, incremental)[[1]]

10. **update_id**  
    - **Description:** The ID of the pipeline update.  
    - **Details:** Unique within a single workspace[[1]]

11. **pipeline_id**  
    - **Description:** The ID of the pipeline.  
    - **Details:** Unique within a single workspace

12. **workspace_id**  
    - **Description:** The ID of the workspace this pipeline belongs to[[1]]

**Example Query:**
To retrieve all these details for successful pipeline updates, you can use:
```sql
SELECT
  workspace_id,
  pipeline_id,
  update_id,
  update_type,
  run_as_user_name,
  request_id,
  trigger_type,
  trigger_details,
  compute,
  period_start_time,
  period_end_time,
  result_state
FROM system.lakeflow.pipeline_update_timeline
WHERE result_state = 'COMPLETED'
```
[[1]]

These columns provide comprehensive metadata for monitoring, auditing, and troubleshooting pipeline updates in Databricks.


The **result_state** column in the `system.lakeflow.pipeline_update_timeline` table indicates the final outcome of a pipeline update. This is a key metric for understanding whether your pipeline update was successful, failed, or canceled.

**Breakdown and Clarification:**

- **What is result_state?**  
  It is a string value that shows the final status of a pipeline update. Possible values include:
  - `COMPLETED`: The pipeline update finished successfully.
  - `FAILED`: The pipeline update encountered an error and did not finish successfully.
  - `CANCELED`: The pipeline update was intentionally stopped before completion[[1]].

- **How is result_state populated for long-running updates?**  
  If a pipeline update runs for more than one hour, Databricks slices the update into hourly intervals and records each interval as a separate row in the table. However, the `result_state` column is only filled in the row that represents the end of the update. The intermediate rows (for each hour) will not have a value in `result_state`—only the final row will show whether the update was `COMPLETED`, `FAILED`, or `CANCELED`[[1]](https://docs.databricks.com/aws/en/admin/system-tables/jobs/ "/docs.databricks.com/aws/en/admin/system-tables/jobs/").

- **Why is this important?**  
  This design allows you to track the progress of long-running updates and only see the final outcome once the update is finished. When analyzing pipeline update history, always look for the row with a populated `result_state` to determine the final outcome of each update.

**Example:**
If a pipeline update takes 3 hours, you will see 3 rows for that update:
- The first two rows (for the first and second hour) will have empty `result_state`.
- The third row (for the last hour) will have `result_state` set to `COMPLETED`, `FAILED`, or `CANCELED`[[1]]

This approach ensures accurate tracking and reporting of pipeline update outcomes, especially for long-running jobs.


The values for the result_state in pipeline_update_timeline table. See https://docs.databricks.com/aws/en/admin/system-tables/jobs#pipeline-result-reference
Return only 'COMPLETED'.

The values for the update_type in the pipeline_update_timeline table. See https://docs.databricks.com/aws/en/admin/system-tables/jobs#pipeline-settings-reference

The values for the trigger_type in the pipeline_update_timeline table. See https://docs.databricks.com/aws/en/admin/system-tables/jobs#pipeline-trigger-type-values

The values for the trigger_type.job_task in the pipeline_update_timeline table. See https://docs.databricks.com/aws/en/admin/system-tables/jobs#pipeline-trigger-type-details

The column-values in system.lakeflow.pipeline table:
The value for workspace_id.
The value for pipeline_id. 
The values for the settings. The settings of the pipeline. See https://docs.databricks.com/aws/en/admin/system-tables/jobs#pipeline-settings-reference
The values for the pipeline_type. See https://docs.databricks.com/aws/en/admin/system-tables/jobs#pipeline-type-values
The value for name. The user-supplied name of the pipeline.
The value for created_by.
The value for run_as.
The value for configuration. 

To join the `system.lakeflow.pipeline_update_timeline` and `system.lakeflow.pipeline` tables in Databricks, 
should use the `workspace_id` and `pipeline_id` columns as join keys, since these uniquely identify a pipeline within 
a workspace and are present in both tables. This approach is fully aligned with Databricks best practices for system tables.

Here’s an optimized and accurate example SQL query:

```sql
SELECT
  put.workspace_id,
  put.pipeline_id,
  put.update_id,
  put.update_type,
  put.result_state,
  put.period_start_time,
  put.period_end_time,
  put.trigger_type,
  put.trigger_details,
  put.request_id,
  put.run_as_user_name,
  put.compute,
  p.name,
  p.pipeline_type,
  p.created_by,
  p.run_as,
  p.settings,
  p.configuration
FROM system.lakeflow.pipeline_update_timeline AS put
INNER JOIN system.lakeflow.pipeline AS p
  ON put.workspace_id = p.workspace_id
  AND put.pipeline_id = p.pipeline_id
```
This query:
- Uses an `INNER JOIN` for performance and accuracy, ensuring only matching pipeline updates and pipeline metadata are returned.
- Selects relevant columns from both tables for comprehensive observability, auditing, and troubleshooting.
- Is optimized for Databricks SQL, leveraging indexed system table columns.

**Best practices:**
- Always filter results as needed (e.g., by date, pipeline type, or result_state) to reduce data scanned and improve performance.
- Use `INNER JOIN` unless specifically need unmatched records (then use `LEFT JOIN`).
- Consider limiting columns to only those needed for analysis to optimize query execution.

Further filter for successful pipeline updates, for example:
```sql
WHERE put.result_state = 'COMPLETED'
```
Add this clause to the query above for targeted results.

This approach ensures a unified view of pipeline update events and their associated pipeline metadata, supporting robust 
monitoring and governance in Databricks.


To join the `system.lakeflow.pipeline_update_timeline` and `system.lakeflow.pipeline` tables and filter for pipeline updates 
where `result_state = 'COMPLETED'`, should use `workspace_id` and `pipeline_id` as join keys. This ensures accuracy and 
optimal performance, as these columns uniquely identify pipelines within a workspace and are indexed for efficient querying.

Here is an optimized Databricks SQL query, fully aligned with Databricks best practices:

```sql
SELECT
  put.workspace_id,
  put.pipeline_id,
  put.update_id,
  put.update_type,
  put.result_state,
  put.period_start_time,
  put.period_end_time,
  put.trigger_type,
  put.trigger_details,
  put.request_id,
  put.run_as_user_name,
  put.compute,
  p.name,
  p.pipeline_type,
  p.created_by,
  p.run_as,
  p.settings,
  p.configuration
FROM system.lakeflow.pipeline_update_timeline AS put
INNER JOIN system.lakeflow.pipeline AS p
  ON put.workspace_id = p.workspace_id
  AND put.pipeline_id = p.pipeline_id
WHERE put.result_state = 'COMPLETED'
```
This query:
- Uses an `INNER JOIN` for performance and to ensure only matching records are returned.
- Filters for updates that have completed successfully.
- Selects relevant columns for comprehensive monitoring, auditing, and troubleshooting.
- Is structured for clarity and maintainability, following Databricks SQL best practices.

Further optimize by selecting only the columns need for specific use case, and by adding additional 
filters (such as date ranges) to reduce the amount of data scanned and improve query performance.

This approach provides a unified and accurate view of completed pipeline updates and their associated pipeline metadata, supporting 
robust operational monitoring and governance in Databricks[[1]](https://docs.databricks.com/aws/en/ldp/develop/ "/docs.databricks.com/aws/en/ldp/develop/").

TODO: Create an optimized and Databricks best-practice-aligned query that joins 
system.lakeflow.pipeline_update_timeline, system.lakeflow.pipeline, system.compute.node_timeline, system.compute.node_types, 
and system.compute.clusters—and filters for pipeline updates where result_state = 'COMPLETED'—follow these steps:

Breakdown & Explanation

1. Join Keys and Relationships

system.lakeflow.pipeline_update_timeline and system.lakeflow.pipeline: Join on workspace_id and pipeline_id (these uniquely identify a pipeline within a workspace).
system.lakeflow.pipeline_update_timeline and system.compute.clusters: Use the cluster identifier from the compute struct in pipeline_update_timeline to join with clusters.
system.compute.clusters and system.compute.node_timeline: Join on cluster_id.
system.compute.node_timeline and system.compute.node_types: Join on node_type_id.

2. Filtering

Only include rows where result_state = 'COMPLETED' to focus on successful pipeline updates.
3. Optimization & Best Practices

Select only the columns you need for analysis.
Use explicit join conditions.
Filter early in the query to reduce data scanned.

Example Query

Below is an optimized SQL query that demonstrates these best practices:

[ TODO ]


Databricks lakeflow declarative pipelines (LDP a framework for creating data pipelines, either batch or streaming in SQL and Python) offers more automation
and less overhead. It is more managed experience (less a customization). For example, the transformations define to perform on data, and 
Lakeflow Declarative Pipelines manages orchestration, monitoring, data quality, errors, and more.
LDP a framework for developing and running batch and streaming data pipelines in SQL and Python. It has more extend capabilities and interoperable with open source Apache
Spark Declarative Pipelines, while it is leveraged the performance-optimized Databricks Runtime.
LDP API (flow) uses the same DataFrame API as Apache Spark and Structured Streaming.

LDP API 
More declarative programming paradigm in nature. It does not aligned more with the imperative/procedural programming patterns.
Less command sequence that dictates how the data should be processed.
Less step-by-step execution: explicitly defines the order of operations.
Minimize the use of control structures: abstract loops, conditionals, and functions manage execution flow.
Auto optimizations and performance tuning.
Less customize in defining explicit steps/sequences of operations to process data.
Focuses more on define what needs to be achieved, less requiring steps-structured approach/logic, leaving the underlying system to determine the best way to execute the task.
Abstracts the how and focuses on defining the desired result.
Avoid specifying step-by-step instructions.
Simplify transformation logic, and the system auto-determines the most efficient execution plan.
Abstraction of execution details: simply describe the desired outcome, not the steps to achieve it.
Automatic optimization: system applies query planning and execution tuning.
Reduced complexity: Removes the need for explicit control structures, improving maintainability.
Leverage declarative programming, including domain-specific and functional programming paradigms.
Automate key aspects of processing management, including orchestration, compute management, monitoring, data quality enforcement, and error handling.
See https://docs.databricks.com/aws/en/ldp/ https://docs.databricks.com/aws/en/ldp/concepts

Databricks recommends starting with the most managed. If it doesn't satisfy the requirements (for example, if it doesn't support data source), 
drop down to the next layer. See the ELT stack https://docs.databricks.com/aws/en/ingestion/#-layers-of-the-etl-stack

When designing data pipelines, must choose between procedural and declarative data processing models (frameworks). This decision impacts workflow complexity,
maintainability, and efficiency. These models have key differences, advantages and challenges, and when to use each approach.

Key differences: see https://docs.databricks.com/aws/en/data-engineering/procedural-vs-declarative#key-differences-procedural-vs-declarative-processing

Use cases:
High-level data processing frameworks such as pipelines.
Scalable, distributed data workloads requiring automated optimizations.
Enable the use cases include data ingestion from sources such as cloud storage (such as Amazon S3) and 
message buses (such as Apache Kafka, Amazon Kinesis, Google Pub/Sub, Azure EventHub, and Apache Pulsar), and 
incremental batch and streaming transformations.

Core capabilities:
Such as pipelines, streaming tables, and materialized views for data processing workflows.

When to choose declarative processing: See https://docs.databricks.com/aws/en/data-engineering/procedural-vs-declarative#when-to-choose-procedural-or-declarative-processing

Benefits of LDP: See https://docs.databricks.com/aws/en/ldp/concepts#what-are-the-benefits-of-sdp

Limitations: See https://docs.databricks.com/aws/en/ldp/limitations

Building with notebooks, JARs, or Python wheels.
Examples:
https://docs.databricks.com/aws/en/ldp/what-is-change-data-capture#examples-of-scd-type-1-and-type-2-processing-with-lakeflow-spark-declarative-pipelines
https://docs.databricks.com/aws/en/ingestion/cloud-object-storage/#automate-etl-with-lakeflow-spark-declarative-pipelines-and-auto-loader

Build an ETL lakeflow declarative pipeline https://docs.databricks.com/aws/en/getting-started/data-pipeline-get-started

Develop ldp using the Lakeflow Pipelines Editor https://docs.databricks.com/aws/en/ldp/multi-file-editor
Develop ldp using the Notebook editor (legacy) https://docs.databricks.com/aws/en/ldp/notebook-devex
Develop ldp using Lakeflow Spark Declarative Pipelines flows https://docs.databricks.com/aws/en/ldp/flows
Develop ldp using Streaming tables https://docs.databricks.com/aws/en/ldp/streaming-tables
Develop ldp using Materialized views https://docs.databricks.com/aws/en/ldp/materialized-views
Develop ldp using Sinks https://docs.databricks.com/aws/en/ldp/ldp-sinks
Develop ldp to load data https://docs.databricks.com/aws/en/ldp/load
Develop ldp to load data from cloud object storage https://docs.databricks.com/aws/en/ldp/load#load-files-from-cloud-object-storage
Develop ldp to load data from an existing table https://docs.databricks.com/aws/en/ldp/load#load-from-an-existing-table
Develop ldp to load data from external systems https://docs.databricks.com/aws/en/ldp/load#load-data-from-external-systems
Develop ldp to Transform data https://docs.databricks.com/aws/en/ldp/transform

Implement data quality with LDP https://docs.databricks.com/aws/en/ldp/expectations

Create pipelines using Python code -> Lakeflow Declarative Pipelines Python language reference
https://docs.databricks.com/aws/en/ldp/developer/#python-development

Configure ldp: See https://docs.databricks.com/aws/en/ldp/configure-pipeline

Upload the sourcecode files in any format to a volume. Files uploaded through the Databricks UI can't exceed 5 GB per file.
To upload files larger than 5 GB, use the Databricks SDK for Python.

Upload using:
Databricks SDK https://docs.databricks.com/aws/en/ingestion/file-upload/upload-to-volume#upload-using-the-databricks-sdk
Databricks CLI https://docs.databricks.com/aws/en/ingestion/file-upload/upload-to-volume#upload-using-the-databricks-cli

Schedule DLT pipeline:
Job schedule adds a pipeline, creates a job for it. The ingestion pipeline is a task within the job. Optionally add more tasks to the job.
https://docs.databricks.com/aws/en/assets/images/saas-connector-orchestration-18ab6ef96dab616bb8d5bfb122f33bd9.png

Compute for ldp:
A serverless for ldp pipeline https://docs.databricks.com/aws/en/ldp/serverless
Serverless pipelines remove most configuration options, as Databricks manages all infrastructure. 
See https://docs.databricks.com/aws/en/ldp/serverless#recommended-configuration-for-serverless-pipelines
Serverless compute limitations https://docs.databricks.com/aws/en/compute/serverless/limitations
A classic compute for ldp pipelines https://docs.databricks.com/aws/en/ldp/configure-compute


Others to know about:

Data history tracking (SCD type 2 & type 1):
See https://docs.databricks.com/aws/en/ingestion/lakeflow-connect/scd
The history tracking setting, also known as the slowly changing dimensions (SCD) setting, determines how to handle changes in data over time.
Turn history tracking off (SCD type 1) to overwrite outdated records as they're updated and deleted in the source. 
Turn history tracking on (SCD type 2) to maintain a history of those changes. Deleting a table or column in the source does NOT delete that data
from the destination, even when SCD type 1 is selected.

How to ingest multiple objects into different schemas and how to ingest one object into multiple target tables?
Pipeline maintaince, see https://docs.databricks.com/aws/en/ingestion/lakeflow-connect/pipeline-maintenance
Clarify, explain, breaks down what, why, when, how the vectorized query?

Clarify, explain, breaks down what, why, when, how the higher-order functions? Make sure it is accurate all-aligned with Databricks.
Higher-order functions in Databricks (specifically in Spark SQL and PySpark) are functions that take other functions as arguments or return them as results.
They are powerful tools for working with complex data types, especially arrays and maps, enabling concise, expressive, and efficient data transformations.
See https://docs.databricks.com/aws/en/semi-structured/higher-order-functions

**What are higher-order functions?**  
Higher-order functions operate on collections (like arrays or maps) and allow you to apply custom logic to each element, 
filter elements, or aggregate results. Examples include `transform`, `filter`, `aggregate`, `exists`, and `reduce`. These functions can take 
lambda expressions or user-defined functions as parameters.

**Why use higher-order functions?**  
- **Expressiveness:** They let you write complex data transformations in a concise way.
- **Performance:** Operations are executed in parallel and optimized by Spark’s engine.
- **Flexibility:** You can apply custom logic to nested or complex data structures without writing verbose code.

**When should you use higher-order functions?**  
- When working with nested data types (arrays, maps, structs) in DataFrames.
- When you need to transform, filter, or aggregate elements within a collection column.
- When you want to avoid writing UDFs (user-defined functions), which can be less efficient.

**How do higher-order functions work in Databricks?**  
- **In SQL:** You use built-in functions like `transform(array, x -> x + 1)`, `filter(array, x -> x > 10)`, or `aggregate(array, 0, (acc, x) -> acc + x)`.
- **In PySpark:** You can use DataFrame methods or SQL expressions to apply these functions.

**Example in Spark SQL:**
```sql
SELECT
  transform(array(1, 2, 3), x -> x * 2) AS doubled
```
This returns `[2, 4, 6]` by applying the lambda `x -> x * 2` to each element.

**Example in PySpark:**
```python
from pyspark.sql.functions import expr

df = spark.createDataFrame([([1, 2, 3],)], ["numbers"])
df.select(expr("transform(numbers, x -> x * 2) as doubled")).show()
```
This also returns `[2, 4, 6]`.

**Summary:**  
Higher-order functions in Databricks are essential for efficient, readable, and scalable data transformations on complex types. They are preferred 
over UDFs for performance and integration with Spark’s optimization engine.

------------------------------------------------------------
DATABRICKS COMPUTE:
See https://docs.databricks.com/aws/en/compute/

Computing resources available on Databricks:
1. Serverless compute for on-demand scaling.
2. Non-serverless (aka classic compute) for customizable resources.

Non-serverless (classic) Compute

Non-serverless compute resources create, configure, and manage/deploy in LOB's AWS cloud account. They are NOT managed by Databricks.
See [1]https://docs.databricks.com/aws/en/compute/configure

Compute plan resources run in AWS account. New compute resources are created/provisioned within JPMC's VPC network in the org LOB's AWS account. See classic/nonserverless compute plane networking https://docs.databricks.com/aws/en/security/network/classic/.

Classic/Non-serverless compute in Databricks can be used in different features, including:

All-purpose compute
Jobs compute
Lakeflow Pipelines
SQL Warehouse (pro and classic types), see https://docs.databricks.com/aws/en/compute/sql-warehouse/warehouse-types/

Serverless Compute

Serverless compute resources run in the compute plane which is managed by Databricks. See [1]https://docs.databricks.com/aws/en/compute/serverless/


Databricks-managed service for on-demand computing resources. Automatically manage compute scales based workload needs.
Databricks automatically upgrades the serverless compute runtime to support enhancements and upgrades to the platform.
Databricks automatically deploys and manage the compute resources allocating.
Workloads run without provisioning any compute resources in cloud (AWS VPC for JPMC) account.

See [1]https://docs.databricks.com/aws/en/security/network/serverless-network-security/#serverless-compute-plane-networking-overview [2]https://docs.databricks.com/aws/en/security/network/serverless-network-security/#what-is-a-network-connectivity-configuration-ncc

Serverless compute can be used in many different features in Databricks, such as:
Serverless Lakeflow Pipelines, see https://docs.databricks.com/aws/en/ldp/serverless
Serverless SQL warehouses, see https://docs.databricks.com/aws/en/compute/sql-warehouse/#what-is-serverless-sql
Serverless jobs, see https://docs.databricks.com/aws/en/jobs/run-serverless-jobs
Serverless GPU compute https://docs.databricks.com/aws/en/compute/serverless/gpu
Serverless Machine Learning (Mosaic AI Model Training) - https://docs.databricks.com/aws/en/machine-learning/train-model/serverless-forecasting

See more details about: compute types https://docs.databricks.com/aws/en/compute/choose-compute


Compute Metrics

The following metrics are NOT exposed in system metadata sources (tables), nor are they accessible via customer-managed storage. 
Instead, Databricks stores them in Databricks-managed storage. This means they cannot retrieve these metrics directly from metadata level; 
must use the Databricks UI compute metrics tool to view them. See https://docs.databricks.com/aws/en/compute/cluster-metrics#hardware-metric-charts

CPU Utilization Metrics:
iowait: Measures the time the CPU spends waiting for I/O operations (like disk or network) to complete.
irq: Time spent handling hardware interrupt requests.
nice: Time used by processes with a positive "niceness" value, which means they have lower priority compared to other tasks.
softirq: Time spent handling software interrupt requests.

Memory Utilization and Swap Metrics:
buffer: Memory used by kernel buffers, which temporarily store data being transferred between devices and processes.
cached: Memory used by the file system cache at the OS level, which helps speed up file access by keeping frequently accessed data in memory.

See more details: [[1]]https://docs.databricks.com/aws/en/compute/cluster-metrics#hardware-metric-charts [2]https://docs.databricks.com/aws/en/optimizations/spark-ui-guide/

Compute System Metadata Sources (Tables)
They include:
- **clusters:** Records the full history of compute configurations over time for non-serverless compute resources, including Lakeflow Pipelines compute.
It tracks details like cluster creation, deletion, node types, and ownership.

- **node_types:** Contains a single record for each available node type, including hardware information. This helps to understand what instance types 
are available and their specifications.

- **node_timeline:** Captures minute-by-minute records of compute utilization metrics for each instance (node) in non-serverless compute.
Each row-record represents one minute of resource usage for a specific node, including CPU, memory, and network metrics.

See [[1]]https://docs.databricks.com/aws/en/admin/system-tables/compute [2]https://docs.databricks.com/aws/en/admin/system-tables/compute#cluster-table-schema [3]https://docs.databricks.com/aws/en/admin/system-tables/compute#node-timeline-table-schema

NOTE: 
Serverless compute pipeline uses two performance modes, either 'optimized' or 'standard' performance modes: Standard performance mode is designed
to reduce costs for workloads where a slightly higher launch latency is acceptable. While the optimized performance mode is enabled faster startup
and execution for time-sensitive workloads. Both modes use the same SKU, but standard performance mode consumes fewer DBUs, reflecting lower compute usage.
See [1]https://docs.databricks.com/aws/en/ldp/serverless#select-a-performance-mode

Non-serverless compute pipeline has two associated compute resources: an update cluster that processes pipeline updates and a maintenance cluster that
runs daily maintenance tasks (including predictive optimization). 
See [1]https://docs.databricks.com/aws/en/ldp/configure-compute#configure-separate-settings-for-the-update-and-maintenance-clusters

- **Non-serverless Lakeflow Pipelines:** There are two associated compute resources:
  - **Update cluster:** Processes pipeline updates.
  - **Maintenance cluster:** Runs daily maintenance tasks, including predictive optimization.

- **Serverless Pipeline Performance Modes:**
  - **Standard performance mode:** Reduces costs with slightly higher launch latency and lower DBU consumption.
  - **Optimized performance mode:** Enables faster startup and execution for time-sensitive workloads, but with higher DBU consumption.
Both modes use the same SKU, but differ in resource usage and cost.

**Scope and Limitations:**
- The compute metadata sources (tables) **only include records for non-serverless compute** (non-serverless Lakeflow pipelines). They **do not contain records for serverless compute**, including serverless Lakeflow Pipelines.

- **Nodes that ran for less than 10 minutes might not appear in the node_timeline** due to data collection granularity.

- **Serverless Lakeflow Pipelines:** When running serverless pipelines, compute instance types—Databricks manages all infrastructure. Most configuration options are removed, and compute metadata do NOT track these compute instance type resources.

See [1]https://docs.databricks.com/aws/en/admin/system-tables/compute#known-limitations [2]https://docs.databricks.com/aws/en/ldp/configure-pipeline [3]https://docs.databricks.com/aws/en/ldp/serverless

**Summary Table:**

| Table Name      | Description | Applies to Non-Serverless | Applies to Serverless |
|-----------------|-------------|--------------------------|----------------------|
| clusters        | Compute config history | Yes | No |
| node_types      | Node type hardware info | Yes | No |
| node_timeline   | Per-minute utilization metrics | Yes | No |


**References to Documentation:**
- The details above are supported by the Databricks documentation on compute system tables, Lakeflow pipeline configuration, and serverless pipeline 
performance modes[[1]](https://docs.databricks.com/aws/en/admin/system-tables/compute/ "/docs.databricks.com/aws/en/admin/system-tables/compute/").

Confirm, clarify, breakdown, explain. Make sure the accurate and well-aligned with Databricks documentation.

Confirm, clarify, breakdown, explain, outline the approach option(s).

TODO: 
Add https://docs.databricks.com/aws/en/ldp/where-is-dlt https://docs.databricks.com/aws/en/ldp/developer/python-ref#dlt-or-pipeline
The product formerly known as Delta Live Tables (DLT) has been updated to Lakeflow Spark Declarative Pipelines (SDP).
Add the billing usage and list_price metadata sources.
Databricks bills based on Databricks units (DBUs), which are units of processing capability per hour based on VM instance type.
See https://docs.databricks.com/aws/en/getting-started/concepts#billing-databricks-units-dbus
Pipelines
Lakeflow Spark Declarative Pipelines provide a declarative framework for building reliable, maintainable, and testable data processing pipelines.
See Lakeflow Spark Declarative Pipelines https://docs.databricks.com/aws/en/ldp/
Workspace
A workspace is an environment for accessing all of your Databricks assets. 
A workspace organizes objects (notebooks, libraries, dashboards, and experiments) into folders and provides access to data objects and computational resources.
See https://docs.databricks.com/aws/en/getting-started/concepts#workspace
Serverless compute, you can't customize the cluster size or instance type. Serverless compute is automatically scaled based on your workload requirements.
Using the system table system.billing.usage. Data profiling is billed under a serverless jobs SKU but does not require your account to be enabled 
for serverless compute for workflows. See https://docs.databricks.com/aws/en/data-quality-monitoring/data-profiling/expense#view-usage-from-the-system-table-systembillingusage
Choose from serverless compute for on-demand scaling, classic compute for customizable resources, or SQL warehouses for optimized analytics.
See https://docs.databricks.com/aws/en/compute/
There are two types of compute planes depending on the compute that uses in Databricks.
Serverless compute resources run in a serverless compute plane in Databricks account.
Classic compute resources runs in your AWS account in what is called the classic compute plane. This refers to the network in your AWS account and its resources.
See https://docs.databricks.com/aws/en/getting-started/high-level-architecture#workspace-architecture https://docs.databricks.com/aws/en/admin/workspace/#what-is-a-workspace
Connect a cloud storage S3 & Databricks Workspace. See https://docs.databricks.com/aws/en/admin/workspace/serverless-workspaces#using-data-in-your-cloud-storage
Classic compute (any compute that is not serverless)
